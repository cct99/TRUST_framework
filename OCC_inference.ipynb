{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2f8c22-6688-48ae-8467-3c69b421c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 1k dataset...\n",
      "\n",
      "Processing 2k dataset...\n",
      "\n",
      "Processing 3k dataset...\n",
      "\n",
      "Processing 4k dataset...\n",
      "\n",
      "Processing 5k dataset...\n",
      "\n",
      "Processing 10k dataset...\n",
      "\n",
      "Processing 15k dataset...\n",
      "\n",
      "Processing 20k dataset...\n",
      "\n",
      "Structured summary saved to: /home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/structured_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "CONFIGS = {\n",
    "    '1k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_1k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_1k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_1k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_1k.csv'\n",
    "    },\n",
    "    '2k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_2k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_2k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_2k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_2k.csv'\n",
    "    },\n",
    "    '3k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_3k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_3k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_3k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_3k.csv'\n",
    "    },\n",
    "    '4k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_4k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_4k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_4k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_4k.csv'\n",
    "    },\n",
    "    '5k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_5k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_5k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_5k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_5k.csv'\n",
    "    },\n",
    "    '10k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_10k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_10k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_10k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_10k.csv'\n",
    "    },\n",
    "    '15k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_15k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_15k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_15k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_15k.csv'\n",
    "    },\n",
    "    '20k': {\n",
    "        'model_path': '/home/ctai42@tntech.edu/OCC/Chao_new_pretrained_mc_15_20k/oc_svm_nu0.01_gamma1.0.joblib',\n",
    "        'scalar_path': '/home/ctai42@tntech.edu/OCC/Pre_Trained_Model/scaler/scaler_mc15_20k.joblib',\n",
    "        'train_data_path': '/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_training_data_20k_mc_15.csv',\n",
    "        'output_csv': '/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15/mc_15_results_Q1_filter_20k.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "TEST_DATA_PATH = \"/home/ctai42@tntech.edu/OCC/Model_training_and_inference_code/new_occ_testing_data_mc_15.csv\"\n",
    "\n",
    "def run_inference_for_size(size_key, config, test_data):\n",
    "    \n",
    "    # Load model and scaler\n",
    "    model = joblib.load(config['model_path'])\n",
    "    scalar = joblib.load(config['scalar_path'])\n",
    "    train_data = pd.read_csv(config['train_data_path'])\n",
    "    \n",
    "    # Extract test data information\n",
    "    image_nums = test_data[\"image_num\"]\n",
    "    actual_classes = test_data[\"actual_class\"]\n",
    "    noise_levels = test_data[\"noise_level\"]\n",
    "    test_features = test_data.drop(columns=[\"image_num\", \"actual_class\", \"noise_level\"])\n",
    "    scaled_test_features = scalar.transform(test_features)\n",
    "\n",
    "    # Calculate Q1 threshold from training data\n",
    "    train_clean_data = train_data[train_data['noise_level'] == 0.0]['top1_mean']\n",
    "    threshold_single = train_clean_data.quantile(0.25)\n",
    "\n",
    "    # Initialize arrays for results\n",
    "    filtered_indices = []\n",
    "    unfiltered_indices = []\n",
    "    all_predictions = np.empty(len(test_data), dtype=object)\n",
    "    stage_used = np.empty(len(test_data), dtype=object)\n",
    "    filter_reason = np.empty(len(test_data), dtype=object)\n",
    "\n",
    "    # Stage 1: Filtering\n",
    "    single_class_filtered = 0\n",
    "    multi_class_count = 0\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        num_classes = sum(1 for j in range(10) if test_features[f'top1_class_{j}_count'].iloc[i] > 0)\n",
    "        \n",
    "        if num_classes == 1:\n",
    "            if test_features['top1_mean'].iloc[i] >= threshold_single:\n",
    "                filtered_indices.append(i)\n",
    "                all_predictions[i] = \"Normal\"\n",
    "                stage_used[i] = \"Stage1\"\n",
    "                filter_reason[i] = \"SingleClassHighConfidence\"\n",
    "                single_class_filtered += 1\n",
    "            else:\n",
    "                unfiltered_indices.append(i)\n",
    "                stage_used[i] = \"Stage2\"\n",
    "                filter_reason[i] = \"SingleClassLowConfidence\"\n",
    "        else:\n",
    "            unfiltered_indices.append(i)\n",
    "            stage_used[i] = \"Stage2\"\n",
    "            filter_reason[i] = \"MultiClass\"\n",
    "            multi_class_count += 1\n",
    "\n",
    "    # Stage 2: OC-SVM Prediction\n",
    "    if unfiltered_indices:\n",
    "        occ_predictions = model.predict(scaled_test_features[unfiltered_indices])\n",
    "        for idx, pred in zip(unfiltered_indices, occ_predictions):\n",
    "            all_predictions[idx] = \"Flagged\" if pred == -1 else \"Normal\"  # Prediction labels\n",
    "\n",
    "    # Create ground truth labels (Comparison between ground truth and prediction labels)\n",
    "    true_labels = np.array([\"Flagged\" if noise > 0 else \"Normal\" for noise in noise_levels])\n",
    "\n",
    "    # Calculate metrics by noise level\n",
    "    noise_level_metrics = {}\n",
    "    if unfiltered_indices:\n",
    "        unfiltered_predictions = all_predictions[unfiltered_indices]\n",
    "        unfiltered_true_labels = true_labels[unfiltered_indices]\n",
    "        unfiltered_noise_levels = noise_levels.iloc[unfiltered_indices]\n",
    "        unfiltered_accuracy = np.mean(unfiltered_predictions == unfiltered_true_labels)\n",
    "\n",
    "        for noise_level in sorted(test_data[\"noise_level\"].unique()):\n",
    "            # Get total samples for this noise level\n",
    "            total_noise_samples = len(test_data[test_data['noise_level'] == noise_level])\n",
    "            \n",
    "            # Get Stage 2 metrics\n",
    "            noise_mask = unfiltered_noise_levels == noise_level\n",
    "            stage2_samples = sum(noise_mask)\n",
    "            if stage2_samples > 0:\n",
    "                noise_preds = unfiltered_predictions[noise_mask]\n",
    "                noise_true = unfiltered_true_labels[noise_mask]\n",
    "                accuracy = np.mean(noise_preds == noise_true)\n",
    "                num_flagged = sum(pred == \"Flagged\" for pred in noise_preds)\n",
    "            else:\n",
    "                accuracy = 0\n",
    "                num_flagged = 0\n",
    "            \n",
    "            filtered_samples = total_noise_samples - stage2_samples\n",
    "            \n",
    "            noise_level_metrics[noise_level] = {\n",
    "                \"total_samples\": total_noise_samples,\n",
    "                \"filtered\": filtered_samples,\n",
    "                \"filtered_percentage\": (filtered_samples / total_noise_samples) * 100,\n",
    "                \"stage2_samples\": stage2_samples,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"flagged\": num_flagged,\n",
    "                \"flagged_percentage\": (num_flagged / stage2_samples * 100) if stage2_samples > 0 else 0\n",
    "            }\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        \"image_num\": image_nums,\n",
    "        \"actual_class\": actual_classes,\n",
    "        \"noise_level\": noise_levels,\n",
    "        \"prediction\": all_predictions,\n",
    "        \"true_label\": true_labels,\n",
    "        \"decision_stage\": stage_used,\n",
    "        \"filter_reason\": filter_reason\n",
    "    })\n",
    "    results.to_csv(config['output_csv'], index=False)\n",
    "\n",
    "    summary = {\n",
    "        'size': size_key,\n",
    "        'threshold': threshold_single,\n",
    "        'total_filtered': len(filtered_indices),\n",
    "        'single_class_high_conf': single_class_filtered,\n",
    "        'multi_class': multi_class_count,\n",
    "        'single_class_low_conf': len(unfiltered_indices) - multi_class_count,\n",
    "        'noise_level_metrics': noise_level_metrics,\n",
    "        'overall_accuracy': unfiltered_accuracy if unfiltered_indices else 0.0,\n",
    "        'total_stage2': len(unfiltered_indices)\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def create_summary_tables(all_summaries):\n",
    "    base_path = \"/home/ctai42@tntech.edu/OCC/New_vary_train_size_results_mc15\"\n",
    "    rows = []\n",
    "    \n",
    "    rows.append(['Section 1: Threshold and Initial Filtering'])\n",
    "    rows.append(['Size', 'Q1_Thres.', 'Filtered', 'Filtered %'])\n",
    "    for summary in all_summaries:\n",
    "        rows.append([\n",
    "            summary['size'],\n",
    "            f\"{summary['threshold']:.4f}\",\n",
    "            summary['total_filtered'],\n",
    "            f\"{(summary['total_filtered'] / 1000) * 100:.1f}\"\n",
    "        ])\n",
    "    \n",
    "    rows.append([])\n",
    "    rows.append([])\n",
    "\n",
    "    rows.append(['Section 2: Performance by Noise Level (Stage 2)'])\n",
    "    \n",
    "    noise_headers = ['Size', 'Metric'] + [f\"NL {level}\" for level in sorted([0.0, 0.1, 0.25, 0.5, 0.75])]\n",
    "    rows.append(noise_headers)\n",
    "    \n",
    "    for summary in all_summaries:\n",
    "        metrics = summary['noise_level_metrics']\n",
    "        \n",
    "        samples_row = [summary['size'], 'Samples']\n",
    "        for noise_level in sorted(metrics.keys()):\n",
    "            samples_row.append(metrics[noise_level]['stage2_samples'])\n",
    "        rows.append(samples_row)\n",
    "        \n",
    "        accuracy_row = ['', 'Accuracy']\n",
    "        for noise_level in sorted(metrics.keys()):\n",
    "            accuracy_row.append(f\"{metrics[noise_level]['accuracy']:.4f}\")\n",
    "        rows.append(accuracy_row)\n",
    "        \n",
    "        flagged_row = ['', '% Flagged']\n",
    "        for noise_level in sorted(metrics.keys()):\n",
    "            flagged_row.append(f\"{metrics[noise_level]['flagged_percentage']:.2f}\")\n",
    "        rows.append(flagged_row)\n",
    "        \n",
    "        rows.append([])\n",
    "    \n",
    "    rows.append([])\n",
    "\n",
    "    rows.append(['Section 3: Overall Performance'])\n",
    "    rows.append(['Size', 'Accuracy', 'S2 Samples'])\n",
    "    for summary in all_summaries:\n",
    "        rows.append([\n",
    "            summary['size'],\n",
    "            f\"{summary['overall_accuracy']:.4f}\",\n",
    "            summary['total_stage2']\n",
    "        ])\n",
    "    \n",
    "    output_file = f'{base_path}/structured_summary.csv'\n",
    "    pd.DataFrame(rows).to_csv(output_file, index=False, header=False)\n",
    "    print(f\"\\nStructured summary saved to: {output_file}\")\n",
    "    \n",
    "def main():\n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "    \n",
    "    # Run inference for each size\n",
    "    all_summaries = []\n",
    "    for size_key, config in CONFIGS.items():\n",
    "        print(f\"\\nProcessing {size_key} dataset...\")\n",
    "        summary = run_inference_for_size(size_key, config, test_data)\n",
    "        all_summaries.append(summary)\n",
    "    \n",
    "    create_summary_tables(all_summaries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a7729-9e09-4ee6-9dc8-b018be16ab66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
